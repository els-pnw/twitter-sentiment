{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a5f77d-373a-4a4f-83f7-c78cc241059e",
   "metadata": {},
   "source": [
    "# Analyze Tweets\n",
    "## With tweepy and pandas\n",
    "### By: Eric L. Sammons <elsammons@gmail.com>\n",
    "---\n",
    "<table style=\"width:75%\">\n",
    "    <caption><strong>Purpose</strong></caption>\n",
    "    <tr>\n",
    "        <td>\n",
    "            This project is to provide the user / reader with an introduction into social media sentiment analysis. We'll leverage the nltk sentiment vader library and dictionary.\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ed/Pandas_logo.svg\" width=\"125\" height=\"75\">\n",
    "            <img src=\"https://static1.squarespace.com/static/538cea80e4b00f1fad490c1b/54668a77e4b00fb778d22a34/54668d8ae4b00fb778d285a2/1416007414694/python_nltk.png\" width=\"125\" height=\"75\">\n",
    "            <img src=\"https://twilio-cms-prod.s3.amazonaws.com/images/twitter-python-logos.width-808.jpg\" width=\"125\" height=\"75\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "    \n",
    "            \n",
    "<table style=\"width:75%\">\n",
    "    <caption><strong>Sentiment Analysis</strong></caption>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Defined as the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"width:75%\">\n",
    "    <caption>\n",
    "        <strong>\n",
    "            Before You Begin\n",
    "        </strong>\n",
    "    </caption>\n",
    "    <tr>\n",
    "        <td>\n",
    "            You will need to have a bearer token from Twitter\n",
    "        </td>\n",
    "        <td>\n",
    "            <a href=\"https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens\">\n",
    "                Using and Generating Bearer Tokens\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"width:75%\">\n",
    "    <caption>\n",
    "        <strong>\n",
    "            Getting Started\n",
    "        </strong>\n",
    "    </caption>\n",
    "    <tr>\n",
    "        <td>\n",
    "            To use this notebook you will need to set up a <strong>.env</strong> file in root folder of this project.  The minimum values required are shown to the right.\n",
    "        </td>\n",
    "        <td>\n",
    "            CONSUMER_KEY=\"xxxxxx\"\n",
    "            CONSUMER_SECRET=\"xxxxxxxxxxx\"\n",
    "            ACCESS_KEY=\"xxxxxxxxxxxx\"\n",
    "            ACCESS_SECRET=\"xxxxxxxxxxxx\"\n",
    "        </td>\n",
    "    </tr>\n",
    " </table>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90db4b7-c122-4e28-97a0-06dd245fae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.1 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\n",
      "tensorflow 2.4.1 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "pythonwhat 2.23.1 requires dill~=0.2.7.1, but you have dill 0.3.3 which is incompatible.\n",
      "pythonwhat 2.23.1 requires jinja2~=2.10, but you have jinja2 3.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install requirements.\n",
    "!pip install -Uqr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb695dc5-15e8-422b-ab34-319b66b768e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/repl/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We'll need these\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "import tweepy\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk import download\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# download vader_lexicon\n",
    "download('vader_lexicon')\n",
    "\n",
    "# Import our helper functions and configs\n",
    "import search\n",
    "from lib.helper_functions import flatten_tweets\n",
    "from lib.helper_functions import calculateCentroid\n",
    "\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98eb0265-dab1-4a5e-af9c-2edb6d7f7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv()) # load our .env file.\n",
    "\n",
    "# Set up our bearer token.\n",
    "auth = tweepy.OAuthHandler(os.environ['CONSUMER_KEY'], os.environ['CONSUMER_SECRET'])\n",
    "auth.set_access_token(os.environ['ACCESS_KEY'], os.environ['ACCESS_SECRET'])\n",
    "\n",
    "# Set api options\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a9cbf9e-cfe5-4d65-940b-50ab0aeaa989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 7 days of tweets.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Searching {search.days} days of tweets.\")\n",
    "today = datetime.date.today() # starting from today\n",
    "p_days= today - datetime.timedelta(days=search.days) # prior days to look back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30a6d78d-a681-48b0-a57a-ba3c8a487722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tweepy client\n",
    "tweets_list = tweepy.Cursor(api.search, \n",
    "                            q=search.search,\n",
    "                            since=str(p_days), \n",
    "                            until=str(today), \n",
    "                            tweet_mode='extended', \n",
    "                            ).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e689f118-e3aa-4d83-90f7-85b45da440df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab some tweets.\n",
    "tweets = []\n",
    "\n",
    "for tweet in tweets_list:\n",
    "    tweets.append(json.dumps(tweet._json))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a5fbf-5798-4393-813c-d6ab2e4c7fb6",
   "metadata": {},
   "source": [
    "With our list of <strong>tweets</strong> we will now create our dataframe.\n",
    "\n",
    "Once the dataframe is created we'll:\n",
    "* Leverage our flatten tweets function to make specific fields more accessible.\n",
    "* Force <strong>created_at</strong> to datetime.\n",
    "* Set the dataframe's index to the <strong>created_at</strong> field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4c63425-0282-4783-aba7-d9d543a3f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(flatten_tweets(tweets))\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])\n",
    "tweets = tweets.set_index('created_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc8b03-8a4a-4b0f-975c-41ede141a4c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "With our tweets captured we are now ready to perform sentiment analysis.\n",
    "\n",
    "---\n",
    "<table style=\"width:75%\">\n",
    "    <th>Value</th>\n",
    "    <th>Translation</th>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Neutral</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>>0</td>\n",
    "        <td>Positive</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><0</td>\n",
    "        <td>Negative</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbd6b89f-05b3-41c0-8042-6cc1338b341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "tweets['scores'] = tweets['full_text'].apply(sid.polarity_scores) # combine with dataframe\n",
    "\n",
    "# Isolate the compound value from scores and create a new column.\n",
    "tweets['compound']  = tweets['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "# Create a new, sentitment, column with pos, neg, neu based on compound.\n",
    "tweets['sentiment'] = tweets['compound'].apply(lambda c: 'pos' if c > 0 else ('neu' if c == 0 else 'neg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc8e6b-3085-47fe-a6c2-62919bf7c30a",
   "metadata": {},
   "source": [
    "Tweets contain a bounding box, an approximate area (in the shape of a rectangle) of where the user tweeted from.  For this bounding box to be populated the user must make location sharing available to the application.\n",
    "\n",
    "In the next step we will take the bounding box and calculate the centroid, providing a simple latitude and longitude value so that we can more easily use mapping features of mapping utilities or libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f0d448f-425c-4be6-9ae7-d0f438fc097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['centroid'] = tweets['place'].apply(calculateCentroid) # calculate centroid\n",
    "tweets[['long', 'lat']] = pd.DataFrame(tweets['centroid'].tolist(), index=tweets.index) # split centroid into long, lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8125360-a701-4e1c-8524-9d6b40868801",
   "metadata": {},
   "source": [
    "We want to set our minimum and maximum date values so that we can use these in our file name.  This will help reduce ambiguity and ensure there's no manual effort here and the consumer of the file can more easily identify the date range in play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e87afc2e-3700-4972-af25-e715ea2ea116",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_created_dt = tweets.index.min().strftime('%Y%m%d')\n",
    "max_created_dt = tweets.index.max().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b82006-cfab-459a-b600-8e71e5a7210a",
   "metadata": {},
   "source": [
    "We write our dataframe out to a csv so that we can download it and use it in an analytics tool like Tableau.  This could just as easily be an s3 bucket; however, accessing a simple s3 file on an s3 bucket can be a bit more difficult than simply accessing the file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "700ce71b-04e0-438a-a03a-d99b1ae53998",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(f'resultsets/tweets_{min_created_dt}_{max_created_dt}.csv', index=True) # write to csv, keep index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f3232-4e48-4050-9a1e-e72f816de6eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13249d7f-f934-46fb-93d1-6a2bb7e40589",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentiment = sentiment_scores.apply(lambda x: x['compound'])\n",
    "# sentiment_resampled = sentiment.resample('1 d').mean()\n",
    "# plt.plot(\n",
    "#     sentiment_resampled.index.day,\n",
    "#     sentiment_resampled, color = 'red'\n",
    "# )\n",
    "\n",
    "\n",
    "# plt.xlabel('Day')\n",
    "# plt.ylabel('Sentiment')\n",
    "# plt.title('Sentiment of Red Hat Tweets')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f3d62-efb4-4bbb-96f8-c3f83edc43ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comp_score_resamp = tweets[tweets['comp_score'] == 'pos']['comp_score'].resample('1 d').count()\n",
    "# plt.plot(\n",
    "#     comp_score_resamp.index.day,\n",
    "#     comp_score_resamp,\n",
    "#     color = 'red'\n",
    "# )\n",
    "\n",
    "# plt.show()\n",
    "# #     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
