{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze Tweets\n",
    "## With tweepy and pandas\n",
    "### By: Eric L. Sammons <elsammons@gmail.com>\n",
    "---\n",
    "### Purpose\n",
    "<table style=\"width:75%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            This project is to provide the user / reader with an introduction into social media sentiment analysis. We'll leverage the nltk sentiment vader library and dictionary.\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ed/Pandas_logo.svg\" width=\"125\" height=\"75\">\n",
    "            <img src=\"https://static1.squarespace.com/static/538cea80e4b00f1fad490c1b/54668a77e4b00fb778d22a34/54668d8ae4b00fb778d285a2/1416007414694/python_nltk.png\" width=\"125\" height=\"75\">\n",
    "            <img src=\"https://twilio-cms-prod.s3.amazonaws.com/images/twitter-python-logos.width-808.jpg\" width=\"125\" height=\"75\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Sentiment Analysis\n",
    "<table style=\"width:75%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            Defined as the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Before You Begin\n",
    "<table style=\"width:75%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            You will need to have a bearer token from Twitter\n",
    "        </td>\n",
    "        <td>\n",
    "            <a href=\"https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens\">\n",
    "                Using and Generating Bearer Tokens\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Getting Started\n",
    "<table style=\"width:75%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            To use this notebook you will need to set up a <strong>.env</strong> file in root folder of this project.  The minimum values required are shown to the right.\n",
    "        </td>\n",
    "        <td>\n",
    "            CONSUMER_KEY=\"xxxxxx\"\n",
    "            CONSUMER_SECRET=\"xxxxxxxxxxx\"\n",
    "            ACCESS_KEY=\"xxxxxxxxxxxx\"\n",
    "            ACCESS_SECRET=\"xxxxxxxxxxxx\"\n",
    "        </td>\n",
    "    </tr>\n",
    " </table>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Install requirements.\n",
    "!pip install -Uqr requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# We'll need these\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "import tweepy\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk import download\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# download vader_lexicon\n",
    "download('vader_lexicon')\n",
    "\n",
    "# Import our helper functions and configs\n",
    "import search\n",
    "from lib.helper_functions import flatten_tweets\n",
    "from lib.helper_functions import calculateCentroid\n",
    "\n",
    "from IPython.core.display import HTML"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/esammons/nltk_data...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "load_dotenv(find_dotenv()) # load our .env file.\n",
    "\n",
    "# Set up our bearer token.\n",
    "auth = tweepy.OAuthHandler(os.environ['CONSUMER_KEY'], os.environ['CONSUMER_SECRET'])\n",
    "auth.set_access_token(os.environ['ACCESS_KEY'], os.environ['ACCESS_SECRET'])\n",
    "\n",
    "# Set api options\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(f\"Searching {search.days} days of tweets.\")\n",
    "today = datetime.date.today() # starting from today\n",
    "p_days= today - datetime.timedelta(days=search.days) # prior days to look back"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Searching 7 days of tweets.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Instantiate the tweepy client\n",
    "tweets_list = tweepy.Cursor(api.search, \n",
    "                            q=search.search,\n",
    "                            since=str(p_days), \n",
    "                            until=str(today), \n",
    "                            tweet_mode='extended', \n",
    "                            ).items()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Let's grab some tweets.\n",
    "tweets = []\n",
    "\n",
    "for tweet in tweets_list:\n",
    "    tweets.append(json.dumps(tweet._json))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With our list of <strong>tweets</strong> we will now create our dataframe.\n",
    "\n",
    "Once the dataframe is created we'll:\n",
    "* Leverage our flatten tweets function to make specific fields more accessible.\n",
    "* Force <strong>created_at</strong> to datetime.\n",
    "* Set the dataframe's index to the <strong>created_at</strong> field"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets = pd.DataFrame(flatten_tweets(tweets))\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])\n",
    "tweets = tweets.set_index('created_at')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With our tweets captured we are now ready to perform sentiment analysis.\n",
    "\n",
    "---\n",
    "<table style=\"width:75%\">\n",
    "    <th>Value</th>\n",
    "    <th>Translation</th>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Neutral</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>>0</td>\n",
    "        <td>Positive</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><0</td>\n",
    "        <td>Negative</td>\n",
    "    </tr>\n",
    "</table>"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "tweets['scores'] = tweets['full_text'].apply(sid.polarity_scores) # combine with dataframe\n",
    "\n",
    "# Isolate the compound value from scores and create a new column.\n",
    "tweets['compound']  = tweets['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "# Create a new, sentitment, column with pos, neg, neu based on compound.\n",
    "tweets['sentiment'] = tweets['compound'].apply(lambda c: 'pos' if c > 0 else ('neu' if c == 0 else 'neg'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tweets contain a bounding box, an approximate area (in the shape of a rectangle) of where the user tweeted from.  For this bounding box to be populated the user must make location sharing available to the application.\n",
    "\n",
    "In the next step we will take the bounding box and calculate the centroid, providing a simple latitude and longitude value so that we can more easily use mapping features of mapping utilities or libraries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets['centroid'] = tweets['place'].apply(calculateCentroid) # calculate centroid\n",
    "tweets[['long', 'lat']] = pd.DataFrame(tweets['centroid'].tolist(), index=tweets.index) # split centroid into long, lat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to set our minimum and maximum date values so that we can use these in our file name.  This will help reduce ambiguity and ensure there's no manual effort here and the consumer of the file can more easily identify the date range in play."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_created_dt = tweets.index.min().strftime('%Y%m%d')\n",
    "max_created_dt = tweets.index.max().strftime('%Y%m%d')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We write our dataframe out to a csv so that we can download it and use it in an analytics tool like Tableau.  This could just as easily be an s3 bucket; however, accessing a simple s3 file on an s3 bucket can be a bit more difficult than simply accessing the file locally."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets.to_csv(f'resultsets/tweets_{min_created_dt}_{max_created_dt}.csv', index=True) # write to csv, keep index"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}